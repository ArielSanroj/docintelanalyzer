# DocsReview - Generador de Informes Financieros/Legales con RAG + LLM

Sistema avanzado de an√°lisis automatizado de documentos financieros y legales usando **RAG (Retrieval-Augmented Generation)** integrado con **LLM** para respuestas m√°s precisas y contextualizadas.

## Caracter√≠sticas Principales

### üîç Sistema RAG Avanzado
- **Recuperaci√≥n inteligente**: Usa embeddings para encontrar informaci√≥n espec√≠fica
- **Chunking inteligente**: Divide documentos en fragmentos optimizados
- **Scoring de relevancia**: Filtra informaci√≥n por pertinencia
- **An√°lisis de intenci√≥n**: Detecta el tipo de consulta autom√°ticamente

### ü§ñ Integraci√≥n LLM Mejorada
- **Contexto espec√≠fico**: El LLM recibe solo informaci√≥n relevante recuperada por RAG
- **Prompts optimizados**: Genera respuestas m√°s precisas y contextualizadas
- **Confianza medible**: Sistema de scoring de confianza para cada respuesta
- **Conversaci√≥n contextual**: Mantiene contexto de interacciones previas

### üìÑ Procesamiento de Documentos
- **PDFs subidos o desde URL**: M√∫ltiples fuentes de entrada
- **OCR autom√°tico**: Extracci√≥n de texto de documentos escaneados
- **An√°lisis con IA**: Usando Llama 3.1 de NVIDIA
- **Res√∫menes estructurados**: Resumen ejecutivo profesional

### üí¨ Chat Inteligente
- **Preguntas espec√≠ficas**: Responde consultas puntuales sobre el documento
- **Informaci√≥n recuperada**: Muestra los fragmentos relevantes encontrados
- **M√©tricas de calidad**: Confianza, relevancia y estad√≠sticas de recuperaci√≥n
- **Historial contextual**: Mantiene contexto de la conversaci√≥n

### üóÑÔ∏è Almacenamiento y Gesti√≥n
- **Base de datos SQLite**: Almacenamiento persistente
- **Interfaz web**: Streamlit para f√°cil uso
- **Gesti√≥n de sesiones**: Estado persistente durante la sesi√≥n

## Instalaci√≥n

1. **Instalar dependencias del sistema**:
```bash
# Para macOS (Apple Silicon)
brew install tesseract tesseract-lang

# Para Ubuntu/Debian
sudo apt-get install tesseract-ocr tesseract-ocr-spa tesseract-ocr-eng
```

2. **Instalar dependencias Python**:
```bash
pip install -r requirements.txt
```

3. **Configurar variables de entorno**:
```bash
# Crear archivo .env
echo "NVIDIA_API_KEY=tu_api_key_aqui" > .env
```

## Uso

### Aplicaci√≥n Web
Ejecutar la aplicaci√≥n:
```bash
streamlit run app.py
```

La aplicaci√≥n estar√° disponible en `http://localhost:8501`

### Demo del Sistema RAG + LLM
Para probar las funcionalidades RAG directamente:
```bash
python rag_example.py
```

## Flujo de Trabajo RAG + LLM

1. **Carga del documento**: El usuario sube un PDF o proporciona una URL
2. **Procesamiento RAG**: 
   - El documento se divide en chunks inteligentes
   - Se generan embeddings para cada chunk
   - Se almacena la informaci√≥n estructurada
3. **Consulta del usuario**: El usuario hace una pregunta espec√≠fica
4. **Recuperaci√≥n inteligente**: 
   - Se analiza la intenci√≥n de la consulta
   - Se recuperan los chunks m√°s relevantes
   - Se optimizan los par√°metros seg√∫n el tipo de consulta
5. **Generaci√≥n mejorada**: 
   - El LLM recibe solo informaci√≥n relevante
   - Se genera una respuesta contextualizada
   - Se calcula un score de confianza
6. **Presentaci√≥n**: Se muestra la respuesta con informaci√≥n de recuperaci√≥n

## Estructura del Proyecto

### Archivos Principales
- `app.py` - Aplicaci√≥n principal con Streamlit y chat RAG
- `docsreview.py` - Motor de procesamiento con LangGraph
- `rag_system.py` - Sistema RAG con embeddings y recuperaci√≥n
- `rag_llm_integration.py` - Integraci√≥n avanzada RAG + LLM
- `rag_example.py` - Demo del sistema RAG + LLM

### Archivos de Soporte
- `database.py` - Funciones de base de datos
- `ocr_utils.py` - Utilidades de OCR
- `requirements.txt` - Dependencias Python (incluye RAG)
- `regulations.db` - Base de datos SQLite (se crea autom√°ticamente)

## Requisitos

- Python 3.8+
- NVIDIA API Key
- Tesseract OCR instalado
- Conexi√≥n a internet para modelos de IA
- Memoria RAM: M√≠nimo 4GB (recomendado 8GB+ para embeddings)

## Caracter√≠sticas T√©cnicas del RAG

### Modelo de Embeddings
- **Modelo**: `sentence-transformers/all-MiniLM-L6-v2`
- **Dimensiones**: 384 dimensiones por embedding
- **Idiomas**: Soporte multiling√ºe (espa√±ol e ingl√©s)

### Par√°metros de Chunking
- **Tama√±o de chunk**: 500 caracteres (configurable)
- **Solapamiento**: 50 caracteres entre chunks
- **Segmentaci√≥n**: Por l√≠mites de oraci√≥n cuando es posible

### Tipos de Consulta Detectados
- **Definici√≥n**: "¬øQu√© es...?", "Definir..."
- **Proceso**: "¬øC√≥mo...?", "Pasos para..."
- **Temporal**: "¬øCu√°ndo...?", "Fecha..."
- **Ubicaci√≥n**: "¬øD√≥nde...?", "Lugar..."
- **Persona**: "¬øQui√©n...?", "Responsable..."
- **Causal**: "¬øPor qu√©...?", "Raz√≥n..."

### M√©tricas de Calidad
- **Score de confianza**: 0.0 - 1.0
- **Relevancia promedio**: Similitud coseno promedio
- **Cobertura**: N√∫mero de chunks relevantes encontrados
- **Especificidad**: Bonus por t√©rminos espec√≠ficos en la consulta

## Ventajas del Sistema RAG + LLM

1. **Precisi√≥n mejorada**: Solo informaci√≥n relevante llega al LLM
2. **Eficiencia**: Menos tokens procesados por el LLM
3. **Transparencia**: Se puede ver qu√© informaci√≥n se us√≥
4. **Escalabilidad**: Funciona con documentos grandes
5. **Contextualizaci√≥n**: Respuestas espec√≠ficas al documento
6. **Confianza medible**: Score de confianza para cada respuesta